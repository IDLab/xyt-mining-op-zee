{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing data stoplocatie detectie\n",
    "In dit script voeren we data preprocessing uit om straks een model te kunnen bouwen die automatisch stoplocaties detecteert met GPS data van schepen. \n",
    "\n",
    "In de ruwe data hebben we informatie over de mmsi van het schip, schipnaam, longitude, latitude, tijd, snelheid, orientatie,  heading, navigatiestatus en shiptype. \n",
    "\n",
    "Met informatie over longitude (x), latitude (y) en tijd (t) creëeren we enkele extra features:\n",
    "    - compass_bearing/ course_over_ground\n",
    "    - VgNr met tijdselement\n",
    " \n",
    "Daarnaast zullen we de dataset resamplen met een frequentie van 3 minuten. Hiervoor is gekozen, omdat het gps-signaal frequenter wordt zodra de snelheid verhoogd. Dit zorgt ervoor dat de afstand tussen XY kleiner wordt, terwijl er harder is gevaren. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import packages\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('bmh')\n",
    "\n",
    "import matplotlib.lines as mlines\n",
    "from shapely.geometry import Polygon #Module for manipulation and analysis of geometric objects in the Cartesian plane.\n",
    "import pandas as pd #This module provides high-performance, easy-to-use data structures and data analysis tools for Python\n",
    "pd.options.mode.chained_assignment = None\n",
    "from shapely.geometry import Point #The Point constructor takes positional coordinate values or point tuple parameters to create a single point.\n",
    "import numpy as np\n",
    "from geopy import distance\n",
    "import csv\n",
    "import time\n",
    "import os\n",
    "import datetime\n",
    "import fnmatch\n",
    "import concurrent.futures\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import NullFormatter\n",
    "import mpld3\n",
    "import folium\n",
    "from geopy.distance import geodesic\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creëer sample uit een groot dataset\n",
    "Om het model goed te kunnen begrijpen willen wij het model testen per schip. Gezien de ruwe dataset meerdere GB's groot is, runnen wij eenmalig per schip de code om het ruwe csv bestand in te lezen en het gekozen schip op te slaan in een nieuwe dataframe. Dit nieuwe dataframe slaan we op als csv, zodat we het nieuwe dataframe kunnen hergebruiken. \n",
    "\n",
    "Mocht het toch interessant zijn om niet per schip, maar naar het totale data bestand te kijken, kun je tot 10.000.000 rijen opslaan in een nieuwe datafram(ongeveer 2/3 dagen aan AIS-data). \n",
    "\n",
    "Uit de eerste 3 dagen van het bestand hebben wij 6000 verschillende schepen geselecteerd. Hieruit hebben wij een schip gekozen om te testen binnen ons model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aan de hand van deze lijst met unique schepen in de dataset, konden wij boten selecteren om het model op te testen\n",
    "# De lijst bevat een groupby die telt hoevaak het schip voorkomt in de eerste twee dagen van de dataset. \n",
    "# unique_ship_list = pd.read_csv(\"random_ship_list.csv\")\n",
    "# print(unique_ship_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### HIERONDER DE METHODE OM DATA IN TE LEZEN. GEZIEN DEZE METHODE ERG LANG DUURT, KUN JE BETER 1x runnen en de output als CSV opslaan\n",
    "### Naast het zoeken naar een specifiek schip, kun je ook de eerste 10.000.000 rijen opslaan als DF. Dit kun je doen door: if index == AANTAL RIJEN: Break;\n",
    "\n",
    "# with open(\"DATAFILE.csv\", newline='') as csvfile:\n",
    "#     csvreader = csv.reader(csvfile);\n",
    "#     header = next(csvreader);\n",
    "#     data = {}\n",
    "#     for h in header:\n",
    "#         data[h] = []\n",
    "#     for index, row in enumerate(csvreader):\n",
    "#          if row[5] == \"MMSI_NUMMER\":\n",
    "#                 for h, v in zip(header, row):\n",
    "#                     data[h].append(v);\n",
    "                \n",
    "# d = pd.DataFrame(data)\n",
    "# d.to_csv(\"raw_MMSI_NUMMER.csv\")\n",
    "###---------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Navigatie status en snelheid bekijken van gekozen schip\n",
    "# h = d.sort_values(\"t_speed\")\n",
    "# print(h.groupby([\"t_speed\"]).count())\n",
    "# print(h.groupby([\"t_navstatus\"]).count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Opschonen data, voeg nieuwe kolommen met extra features toe. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Inlezen csv bestand met 1 schip en selecteer juiste kolommen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inlezen bestand\n",
    "df = pd.read_csv(\"sample.csv\",  usecols=[\"t_mmsi\", \"t_name\", \"t_updatetime\", \"t_longitude\", \"t_latitude\", \"t_speed\", \"t_orientation\", \"t_navstatus\", \"p_shiptype\"] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Omzetten UTC tijd en tijdseenheden in aparte kolommen "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Omzetten van UTC tijd naar locale tijd (Amsterdam) en verwijderen overbodige kolommen\n",
    "df[\"Time\"] = pd.DatetimeIndex(df['t_updatetime']).time\n",
    "df['Date'] = pd.DatetimeIndex(df['t_updatetime']).date\n",
    "df['DateTime_UTC'] = df.apply(lambda r : pd.datetime.combine(r['Date'],r['Time']),1)\n",
    "df['DateTime_Local'] = df['DateTime_UTC'].dt.tz_localize('utc').dt.tz_convert('Europe/Amsterdam')\n",
    "#Verwijder niet nodige tijdskolommen\n",
    "del df[\"DateTime_UTC\"]\n",
    "del df[\"t_updatetime\"]\n",
    "del df['Time']\n",
    "del df['Date']\n",
    "\n",
    "# splitten tijdgegevens in kolommen\n",
    "df['Date'] = pd.DatetimeIndex(df['DateTime_Local']).date\n",
    "df['Time'] = pd.DatetimeIndex(df['DateTime_Local']).time\n",
    "df['DateTime'] = df[\"DateTime_Local\"]\n",
    "\n",
    "## Hieronder nog meer code om tijd op te splitten\n",
    "df['Year'] = pd.DatetimeIndex(df['DateTime']).year\n",
    "df['Month'] = pd.DatetimeIndex(df['DateTime']).month\n",
    "df['Day'] = pd.DatetimeIndex(df['DateTime']).day\n",
    "df['Weeknr'] = pd.DatetimeIndex(df['DateTime']).week\n",
    "df['Weekdag'] = pd.DatetimeIndex(df['DateTime']).weekday\n",
    "df['Hour'] = pd.DatetimeIndex(df['DateTime']).hour\n",
    "df[\"minute\"] = pd.DatetimeIndex(df[\"DateTime\"]).minute\n",
    "df[\"sec\"] = pd.DatetimeIndex(df[\"DateTime\"]).second\n",
    "\n",
    "#Verwijderen dubbele tijdskolommen \n",
    "del df['DateTime_Local']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Creëer volgnummer met tijdselement\n",
    "In het volgende script gaan we data clusteren. Gezien \"date-format\" geen geschikt formaat is om mee te clusteren, berekenen we tijd om in seconden. Dit zorgt voor groot onleesbaar getal, dus kijken we naar het verschil en tellen we dit verschil bij elkaar op. De uitkomst hiervan noemen we het VgNr met tijdselement.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bereken datum in seconden\n",
    "Year_sec = df[\"Year\"] * 365 * 24 * 60 * 60\n",
    "Month_sec = df[\"Month\"] * 31 * 24 * 60 * 60\n",
    "Day_sec = df[\"Day\"] * 24 * 60 * 60\n",
    "Hour_sec = df[\"Hour\"] * 60 * 60\n",
    "min_sec = df[\"minute\"] * 60\n",
    "sec_sec = df[\"sec\"]\n",
    "\n",
    "df[\"date_in_sec\"] = Year_sec + Month_sec + Day_sec + Hour_sec + min_sec + sec_sec\n",
    "\n",
    "# Creer volgnummer met tijdselement\n",
    "# Bereken verschil tussen tijden en tel ze daarna bij elkaar op\n",
    "df[\"VgNr\"] = df[\"date_in_sec\"].diff().cumsum()\n",
    "# verwijder de NAN en zet float om in integers\n",
    "df = df.dropna(subset = [\"VgNr\"]).reset_index(drop=True)\n",
    "df[\"VgNr\"] = df[\"VgNr\"].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tel aantal berichten per minuut\n",
    "Een andere feature die we kunnen maken met tijd is het aantal berichten per minuut. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maak kopie van dataframe vanwege latere merge\n",
    "df_copy = df.copy()  \n",
    "# Tel berichten per minuut door aantal rijen per minuut te tellen in df met een groupby. Merge daarna de resultaten met huidige df. \n",
    "count_messages_per_minute = df_copy.groupby([\"Date\", \"Hour\", \"minute\"])[\"t_mmsi\"].count().reset_index(name=\"messages_per_minute\")\n",
    "df_copy_2 = pd.merge(df_copy, count_messages_per_minute, left_on=[\"Date\", \"Hour\", \"minute\"], right_on=[\"Date\", \"Hour\", \"minute\"])\n",
    "# Je kunt ditzelfde doen per uur\n",
    "count_messages_per_hour = df_copy_2.groupby([\"Date\", \"Hour\"])[\"t_mmsi\"].count().reset_index(name=\"messages_per_hour\")\n",
    "df = pd.merge(df_copy_2, count_messages_per_hour, left_on=[\"Date\", \"Hour\"], right_on=[\"Date\", \"Hour\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Resample de data\n",
    "Het AIS-signaal zendt frequenter data wanneer de snelheid van de boot versneld. Daardoor wordt de afstand tussen varende punten kleiner. Dit is niet goed voor het model, gezien we juist een grotere afstand willen tussen varende punten in vergelijking met stilliggende punten. Daarom samplen we de data naar 3 minuten. Er is gekozen voor 3 minuten, gezien dat de frequentie van het AIS-signaal is wanneer een schip stil ligt. \n",
    "\n",
    "Er zijn binnen Pandas twee manieren om te samplen: Grouper en Resample. Bij Grouper gooi je punten weg die binnen het geselecteerde tijdsinterval liggen, daarnaast behoudt je de originele tijd. Bij Resample verdeel je de data over gelijke tijdsintervallen (bijv 3min), dus behoud niet de orginele tijd. Punten die binnen de tijdsintervallen liggen, worden verwijderd. Je kunt o.a. per tijdinterval kiezen of je de sum, mean, eerste waarde of laatste waarde van de punten binnen het tijdsinterval pakt. Daarnaast voegt hij puntjes toe, wanneer de tijdsinterval langer is dan aangegeven. Deze punten krijgen een tijdindex, maar de rest van de kolommen krijgen de value NaN. Het is mogelijk om deze NaN te vullen met bijv de waarde die voor of na de NaN te vinden is. \n",
    "\n",
    "Binnen deze opdracht hebben we gekozen voor de functie Resample. Op stoplocaties kan het zijn dat de AIS heeft uitgestaan waardoor het verschil in tijd tussen twee punten kan oplopen tot meerdere uren. Door hier meer punten toe te voegen (met alle dezelfde waarde als het punt daarvoor of daarna), versterk je het stop signaal. Dit is positief voor het resultaat. \n",
    "\n",
    "Let op: wanneer het signaal tijdens het varen wegvalt, zullen hier ook extra punten worden toevoegd. Controleer of dit tot problemen leidt (voor de schepen die wij hebben getest, leidde dit niet tot problemen). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nieuwe df gezien latere merge \n",
    "df_sample = df.copy()\n",
    "\n",
    "# Gebruik Resample om gelijke tijdsintervallen te creëren van 3 minuten. Gebruik max() om de laatste rij te selecteren voor de tijdsintval (je kunt hier ook bijv. sum, mean of min() gebruiken)\n",
    "df_sample = df_sample.set_index(\"DateTime\").resample('3T').max()\n",
    "# Gebruik fillna om een waarde toe te kennen aan de punten die zijn toegevoegd (deze gegevens hebben een NA gekregen). De backfill methode gebuikt de volgende valide waarde uit de dataset om de gab te vullen. Je kunt ook ffill gebruiken , deze gebruik juist de laatste valide waarde uit de dataset om de gab te dichten. \n",
    "df_sample = df_sample.fillna(method=\"backfill\")\n",
    "\n",
    "# Andere methode om te samplen is via Grouper. Deze methode behoudt de origele tijd, waardoor de tijdsintervallen niet allemaal gelijk zijn (maar wel een eerlijker beeld geven). Volgens mij is het niet mogelijk om te upsamplen via grouper (kan hier geen documentatie over vinden). Vandaar niet interessant voor ons project. \n",
    "# df_sample = df_sample.groupby(['t_name', pd.Grouper(key='DateTime_index', freq='3Min')]).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Met de resample tool worden de string-kolommen verwijderd, met merge weer toevoegen\n",
    "\n",
    "# Ivm de merge ontstaan er dubbele kolommen, deze kun je verwijderen met de onderstaande functies\n",
    "def drop_y(df):\n",
    "    to_drop = [x for x in df if x.endswith('_y')]\n",
    "    df.drop(to_drop, axis=1, inplace=True)\n",
    "    \n",
    "def rename_x(df):\n",
    "    for col in df:\n",
    "        if col.endswith('_x'):\n",
    "            df.rename(columns={col:col.rstrip('_x')}, inplace=True)\n",
    "            \n",
    "# df_merge = pd.merge(df_sample, df, left_index=True, how=\"inner\", on = \"VgNr\")\n",
    "df_merge = pd.merge(df_sample, df, how=\"left\", on = \"VgNr\")\n",
    "drop_y(df=df_merge)\n",
    "rename_x(df=df_merge)\n",
    "\n",
    "df = df_merge.copy()\n",
    "# print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Naast VgNr met tijdselement ook ID meegeven\n",
    "We willen testen hoe het clustermodel omgaat met tijd (VgNr) en hoe deze omgaat met volgorde van punten zonder tijdselement (ID). Dit doen wij omdat er soms veel tijd zit tussen de punten wanneer een schip stil ligt. Dit is nadelig voor het model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Geef elke rij een id door index naar een kolom om te zetten\n",
    "df = df.reset_index()\n",
    "df[\"ID\"] = df.index.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creëer features met koers\n",
    "Het is mogelijk om tussen twee punten de hoek te berekenen en dit om te zetten in \"course over ground\"/\"compass bearing\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1/3 feature koers: eerste afgeleide van t_orientation tussen twee punten\n",
    "\n",
    "## bereken eerste afgeleide + zet om in absolute\n",
    "df[\"t_orientation_dff\"] = df[\"t_orientation\"].diff(1).abs()\n",
    "## Gezien 0 / 360 allebei het noorden zijn trekken we 360 graden van het totaal bij punten boven de 180 graden, zodat met eenheden onder de 180 graden\n",
    "df.loc[df[\"t_orientation_dff\"] > 180, \"t_orientation_dff\"] = (df.loc[df[\"t_orientation_dff\"] > 180, \"t_orientation_dff\"] - 360).abs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2/3 feature koers: bereken hoek en graden tussen twee punten\n",
    "\n",
    "# verschil lat1 en lat2\n",
    "x1 = df.t_latitude\n",
    "x2 = df.t_latitude.shift(-1) \n",
    "deltax = x2 -x1\n",
    "\n",
    "# verschil lon1 en lon2\n",
    "y1 = df.t_longitude\n",
    "y2 = df.t_longitude.shift(-1)\n",
    "deltay = y2 - y1\n",
    "\n",
    "# bereken hoek \n",
    "df[\"angle_rad\"] = np.arctan2(deltay,deltax).abs()\n",
    "\n",
    "# bereken graden\n",
    "df[\"angle_deg\"] = df[\"angle_rad\"]*180.0/np.pi\n",
    "\n",
    "# Bereken eerste afgeleide + zet om in absolute\n",
    "df[\"angle_deg_diff\"] = df[\"angle_deg\"].diff().abs()\n",
    "## Gezien 0 / 360 allebei het noorden zijn trekken we 360 graden van het totaal bij punten boven de 180 graden, zodat met eenheden onder de 180 graden\n",
    "df.loc[df[\"angle_deg_diff\"] > 180, \"angle_deg_diff\"] = (df.loc[df[\"angle_deg_diff\"] > 180, \"angle_deg_diff\"] - 360).abs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 3/3 feature koers: ingewikkelder formule om compass bearing uit te rekenen\n",
    "## Ik snap niet helemaal wat hij doet\n",
    "\n",
    "# De eerste functie is een wat uit\n",
    "def calculate_initial_compass_bearing(df):\n",
    "#     Calculates the bearing between two points.\n",
    "#     The formulae used is the following:\n",
    "#         θ = atan2(sin(Δlong).cos(lat2),\n",
    "#                   cos(lat1).sin(lat2) − sin(lat1).cos(lat2).cos(Δlong))\n",
    "#     :Parameters:\n",
    "#       - `pointA: The tuple representing the latitude/longitude for the\n",
    "#         first point. Latitude and longitude must be in decimal degrees\n",
    "#       - `pointB: The tuple representing the latitude/longitude for the\n",
    "#         second point. Latitude and longitude must be in decimal degrees\n",
    "#     :Returns:\n",
    "#       The bearing in degrees\n",
    "#     :Returns Type:\n",
    "#       float\n",
    "\n",
    "#     if (type(pointA) != tuple) or (type(pointB) != tuple):\n",
    "#         raise TypeError(\"Only tuples are supported as arguments\")\n",
    "#     lat = df[]\n",
    "    lat = df[\"t_latitude\"].iloc[:-1]\n",
    "    lat2 = df[\"t_latitude\"].shift(-1).dropna()\n",
    "    lat = np.asarray(lat)\n",
    "    lat2 = np.asarray(lat2)\n",
    "    lon = df[\"t_longitude\"].iloc[:-1]\n",
    "    lon2 = df[\"t_longitude\"].shift(-1).dropna()\n",
    "    lon = np.asarray(lon)\n",
    "    lon2 = np.asarray(lon2)\n",
    "\n",
    "    diffLong = np.radians(lon2 - lon)\n",
    "\n",
    "    x = np.sin(diffLong) * np.cos(lat2)\n",
    "    y = np.cos(lat) * np.sin(lat2) - (np.sin(lat)\n",
    "            * np.cos(lat2) * np.cos(diffLong))\n",
    "\n",
    "    initial_bearing = np.arctan2(x, y)\n",
    "\n",
    "#     # Now we have the initial bearing but math.atan2 return values\n",
    "#     # from -180° to + 180° which is not what we want for a compass bearing\n",
    "#     # The solution is to normalize the initial bearing as shown below\n",
    "    initial_bearing = np.degrees(initial_bearing)\n",
    "    df = df.iloc[:-1]\n",
    "    df[\"compass_bearing\"] = (initial_bearing + 360) % 360\n",
    "    \n",
    "    # Bereken eerste afgeleide + zet om in absolute\n",
    "    df[\"compass_dff\"] = df[\"compass_bearing\"].diff().abs()\n",
    "    ## Gezien 0 / 360 allebei het noorden zijn trekken we 360 graden van het totaal bij punten boven de 180 graden, zodat met eenheden onder de 180 graden\n",
    "    df.loc[df[\"compass_dff\"] > 180, \"compass_dff\"] = (df.loc[df[\"compass_dff\"] > 180, \"compass_dff\"] - 360).abs()\n",
    "\n",
    "    return(df)\n",
    "\n",
    "df = calculate_initial_compass_bearing(df=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sla csv op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## NAN door verschil berekenen koers\n",
    "df = df.dropna(subset = [\"t_orientation_dff\"]).reset_index(drop=True)\n",
    "## Wellicht zitten NAN's in de t_latitude/ t_longitude/ t_navstatus\n",
    "df = df.dropna(subset = [\"t_latitude\", \"t_longitude\"]).reset_index(drop=True)\n",
    "df = df.dropna(subset = [\"t_navstatus\"]).reset_index(drop=True)\n",
    "df[\"t_navstatus\"] = df[\"t_navstatus\"].astype(int)\n",
    "## Maak csv van preprocessing\n",
    "df.to_csv(\"sample_AIS_preprocessing.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
