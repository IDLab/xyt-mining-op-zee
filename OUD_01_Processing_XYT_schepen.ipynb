{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing data stoplocatie detectie\n",
    "In dit script voeren we data preprocessing uit om straks een model te kunnen bouwen die automatisch stoplocaties detecteert met GPS data van schepen. \n",
    "\n",
    "In de ruwe data hebben we informatie over de mmsi van het schip, schipnaam, longitude, latitude, tijd, snelheid, orientatie,  heading, navigatiestatus en shiptype. \n",
    "\n",
    "Met informatie over longitude (x), latitude (y) en tijd (t) creëeren we enkele extra features:\n",
    "    - compass_bearing/ course_over_ground\n",
    "    - VgNr met tijdselement\n",
    " \n",
    "Daarnaast zullen we de dataset resamplen met een frequentie van 3 minuten. Hiervoor is gekozen, omdat het gps-signaal frequenter wordt zodra de snelheid verhoogd. Dit zorgt ervoor dat de afstand tussen XY kleiner wordt, terwijl er harder is gevaren. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import packages\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('bmh')\n",
    "\n",
    "import matplotlib.lines as mlines\n",
    "from shapely.geometry import Polygon #Module for manipulation and analysis of geometric objects in the Cartesian plane.\n",
    "import pandas as pd #This module provides high-performance, easy-to-use data structures and data analysis tools for Python\n",
    "pd.options.mode.chained_assignment = None\n",
    "from shapely.geometry import Point #The Point constructor takes positional coordinate values or point tuple parameters to create a single point.\n",
    "import numpy as np\n",
    "from geopy import distance\n",
    "import csv\n",
    "import time\n",
    "import os\n",
    "import datetime\n",
    "import fnmatch\n",
    "import concurrent.futures\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import NullFormatter\n",
    "import mpld3\n",
    "import folium\n",
    "from geopy.distance import geodesic\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creëer sample uit een groot dataset\n",
    "Om het model goed te kunnen begrijpen willen wij het model testen per schip. Gezien de ruwe dataset meerdere GB's groot is, runnen wij eenmalig per schip de code om het ruwe csv bestand in te lezen en het gekozen schip op te slaan in een nieuwe dataframe. Dit nieuwe dataframe slaan we op als csv, zodat we het nieuwe dataframe kunnen hergebruiken. \n",
    "\n",
    "Mocht het toch interessant zijn om niet per schip, maar naar het totale data bestand te kijken, kun je tot 10.000.000 rijen opslaan in een nieuwe datafram(ongeveer 2/3 dagen aan AIS-data). \n",
    "\n",
    "Uit de eerste 3 dagen van het bestand hebben wij 6000 verschillende schepen geselecteerd. Hieruit hebben wij een schip gekozen om te testen binnen ons model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aan de hand van deze lijst met unique schepen in de dataset, konden wij boten selecteren om het model op te testen\n",
    "# De lijst bevat een groupby die telt hoevaak het schip voorkomt in de eerste twee dagen van de dataset. \n",
    "# unique_ship_list = pd.read_csv(\"random_ship_list.csv\")\n",
    "# print(unique_ship_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### HIERONDER DE METHODE OM DATA IN TE LEZEN. GEZIEN DEZE METHODE ERG LANG DUURT, KUN JE BETER 1x runnen en de output als CSV opslaan\n",
    "### Naast het zoeken naar een specifiek schip, kun je ook de eerste 10.000.000 rijen opslaan als DF. Dit kun je doen door: if index == AANTAL RIJEN: Break;\n",
    "\n",
    "# with open(\"DATAFILE.csv\", newline='') as csvfile:\n",
    "#     csvreader = csv.reader(csvfile);\n",
    "#     header = next(csvreader);\n",
    "#     data = {}\n",
    "#     for h in header:\n",
    "#         data[h] = []\n",
    "#     for index, row in enumerate(csvreader):\n",
    "#          if row[5] == \"MMSI_NUMMER\":\n",
    "#                 for h, v in zip(header, row):\n",
    "#                     data[h].append(v);\n",
    "                \n",
    "# d = pd.DataFrame(data)\n",
    "# d.to_csv(\"raw_MMSI_NUMMER.csv\")\n",
    "###---------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Navigatie status en snelheid bekijken van gekozen schip\n",
    "# h = d.sort_values(\"t_speed\")\n",
    "# print(h.groupby([\"t_speed\"]).count())\n",
    "# print(h.groupby([\"t_navstatus\"]).count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Opschonen data, voeg nieuwe kolommen met extra features toe. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Inlezen csv bestand met 1 schip en selecteer juiste kolommen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inlezen bestand\n",
    "df = pd.read_csv(\"sample.csv\",  usecols=[\"t_mmsi\", \"t_name\", \"t_updatetime\", \"t_longitude\", \"t_latitude\", \"t_speed\", \"t_orientation\", \"t_navstatus\", \"p_shiptype\"] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Omzetten UTC tijd en tijdseenheden in aparte kolommen "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Omzetten van UTC tijd naar locale tijd (Amsterdam) en verwijderen overbodige kolommen\n",
    "df[\"Time\"] = pd.DatetimeIndex(df['t_updatetime']).time\n",
    "df['Date'] = pd.DatetimeIndex(df['t_updatetime']).date\n",
    "df['DateTime_UTC'] = df.apply(lambda r : pd.datetime.combine(r['Date'],r['Time']),1)\n",
    "df['DateTime_Local'] = df['DateTime_UTC'].dt.tz_localize('utc').dt.tz_convert('Europe/Amsterdam')\n",
    "#Verwijder niet nodige tijdskolommen\n",
    "del df[\"DateTime_UTC\"]\n",
    "del df[\"t_updatetime\"]\n",
    "del df['Time']\n",
    "del df['Date']\n",
    "\n",
    "# splitten tijdgegevens in kolommen\n",
    "df['Date'] = pd.DatetimeIndex(df['DateTime_Local']).date\n",
    "df['Time'] = pd.DatetimeIndex(df['DateTime_Local']).time\n",
    "df['DateTime'] = df[\"DateTime_Local\"]\n",
    "\n",
    "## Hieronder nog meer code om tijd op te splitten\n",
    "df['Year'] = pd.DatetimeIndex(df['DateTime']).year\n",
    "df['Month'] = pd.DatetimeIndex(df['DateTime']).month\n",
    "df['Day'] = pd.DatetimeIndex(df['DateTime']).day\n",
    "df['Weeknr'] = pd.DatetimeIndex(df['DateTime']).week\n",
    "df['Weekdag'] = pd.DatetimeIndex(df['DateTime']).weekday\n",
    "df['Hour'] = pd.DatetimeIndex(df['DateTime']).hour\n",
    "df[\"minute\"] = pd.DatetimeIndex(df[\"DateTime\"]).minute\n",
    "df[\"sec\"] = pd.DatetimeIndex(df[\"DateTime\"]).second\n",
    "\n",
    "#Verwijderen dubbele tijdskolommen \n",
    "del df['DateTime_Local']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Creëer volgnummer met tijdselement\n",
    "In het volgende script gaan we data clusteren. Gezien \"date-format\" geen geschikt formaat is om mee te clusteren, berekenen we tijd om in seconden. Dit zorgt voor groot onleesbaar getal, dus kijken we naar het verschil en tellen we dit verschil bij elkaar op. De uitkomst hiervan noemen we het VgNr met tijdselement.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bereken datum in seconden\n",
    "Year_sec = df[\"Year\"] * 365 * 24 * 60 * 60\n",
    "Month_sec = df[\"Month\"] * 31 * 24 * 60 * 60\n",
    "Day_sec = df[\"Day\"] * 24 * 60 * 60\n",
    "Hour_sec = df[\"Hour\"] * 60 * 60\n",
    "min_sec = df[\"minute\"] * 60\n",
    "sec_sec = df[\"sec\"]\n",
    "\n",
    "df[\"date_in_sec\"] = Year_sec + Month_sec + Day_sec + Hour_sec + min_sec + sec_sec\n",
    "\n",
    "# Creer volgnummer met tijdselement\n",
    "# Bereken verschil tussen tijden en tel ze daarna bij elkaar op\n",
    "df[\"VgNr\"] = df[\"date_in_sec\"].diff().cumsum()\n",
    "# verwijder de NAN en zet float om in integers\n",
    "df = df.dropna(subset = [\"VgNr\"]).reset_index(drop=True)\n",
    "df[\"VgNr\"] = df[\"VgNr\"].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tel aantal berichten per minuut\n",
    "Een andere feature die we kunnen maken met tijd is het aantal berichten per minuut. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maak kopie van dataframe vanwege latere merge\n",
    "df_copy = df.copy()  \n",
    "# Tel berichten per minuut door aantal rijen per minuut te tellen in df met een groupby. Merge daarna de resultaten met huidige df. \n",
    "count_messages_per_minute = df_copy.groupby([\"Date\", \"Hour\", \"minute\"])[\"t_mmsi\"].count().reset_index(name=\"messages_per_minute\")\n",
    "df_copy_2 = pd.merge(df_copy, count_messages_per_minute, left_on=[\"Date\", \"Hour\", \"minute\"], right_on=[\"Date\", \"Hour\", \"minute\"])\n",
    "# Je kunt ditzelfde doen per uur\n",
    "count_messages_per_hour = df_copy_2.groupby([\"Date\", \"Hour\"])[\"t_mmsi\"].count().reset_index(name=\"messages_per_hour\")\n",
    "df = pd.merge(df_copy_2, count_messages_per_hour, left_on=[\"Date\", \"Hour\"], right_on=[\"Date\", \"Hour\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Resample de data\n",
    "Het AIS-signaal zendt frequenter data wanneer de snelheid van de boot versneld. Daardoor wordt de afstand tussen varende punten kleiner. Dit is niet goed voor het model, gezien we juist een grotere afstand willen tussen varende punten in vergelijking met stilliggende punten. Daarom samplen we de data naar 3 minuten. Er is gekozen voor 3 minuten, gezien dat de frequentie van het AIS-signaal is wanneer een schip stil ligt. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nieuwe df gezien latere merge + nieuwe datatime kolom gezien deze nu index wordt\n",
    "df[\"DateTime_index\"] = df[\"DateTime\"].copy()\n",
    "df_sample = df.copy()\n",
    "\n",
    "# Nu gaan we resamplen. Met \".max()\" geven we aan dat we de laatste rij binnen 3 min willen opslaan in nieuwe DF \n",
    "df_sample = df_sample.groupby(['t_name', pd.Grouper(key='DateTime_index', freq='3Min')]).max()\n",
    "\n",
    "## Andere manier om te samplen is via df.resample()\n",
    "# new_sample = df.resample(\"3T\", on=\"DateTime_sample\")\n",
    "# df = new_sample.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Met de resample tool worden de string-kolommen verwijderd, met merge weer toevoegen\n",
    "\n",
    "# Ivm de merge ontstaan er dubbele kolommen, deze kun je verwijderen met de onderstaande functies\n",
    "def drop_y(df):\n",
    "    to_drop = [x for x in df if x.endswith('_y')]\n",
    "    df.drop(to_drop, axis=1, inplace=True)\n",
    "    \n",
    "def rename_x(df):\n",
    "    for col in df:\n",
    "        if col.endswith('_x'):\n",
    "            df.rename(columns={col:col.rstrip('_x')}, inplace=True)\n",
    "            \n",
    "df_merge = pd.merge(df_sample, df, left_index=True, how=\"inner\", on=\"VgNr\")\n",
    "drop_y(df=df_merge)\n",
    "rename_x(df=df_merge)\n",
    "\n",
    "df = df_merge.copy()\n",
    "# print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Naast VgNr met tijdselement ook ID meegeven\n",
    "We willen testen hoe het clustermodel omgaat met tijd (VgNr) en hoe deze omgaat met volgorde van punten zonder tijdselement (ID). Dit doen wij omdat er soms veel tijd zit tussen de punten wanneer een schip stil ligt. Dit is nadelig voor het model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Geef elke rij een id door index naar een kolom om te zetten\n",
    "df = df.reset_index()\n",
    "df[\"ID\"] = df.index.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creëer features met koers\n",
    "Het is mogelijk om tussen twee punten de hoek te berekenen en dit om te zetten in \"course over ground\"/\"compass bearing\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1/3 feature koers: eerste afgeleide van t_orientation tussen twee punten\n",
    "\n",
    "## bereken eerste afgeleide + zet om in absolute\n",
    "df[\"t_orientation_dff\"] = df[\"t_orientation\"].diff(1).abs()\n",
    "## Gezien 0 / 360 allebei het noorden zijn trekken we 360 graden van het totaal bij punten boven de 180 graden, zodat met eenheden onder de 180 graden\n",
    "df.loc[df[\"t_orientation_dff\"] > 180, \"t_orientation_dff\"] = (df.loc[df[\"t_orientation_dff\"] > 180, \"t_orientation_dff\"] - 360).abs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2/3 feature koers: bereken hoek en graden tussen twee punten\n",
    "\n",
    "# verschil lat1 en lat2\n",
    "x1 = df.t_latitude\n",
    "x2 = df.t_latitude.shift(-1) \n",
    "deltax = x2 -x1\n",
    "\n",
    "# verschil lon1 en lon2\n",
    "y1 = df.t_longitude\n",
    "y2 = df.t_longitude.shift(-1)\n",
    "deltay = y2 - y1\n",
    "\n",
    "# bereken hoek \n",
    "df[\"angle_rad\"] = np.arctan2(deltay,deltax).abs()\n",
    "\n",
    "# bereken graden\n",
    "df[\"angle_deg\"] = df[\"angle_rad\"]*180.0/np.pi\n",
    "\n",
    "# Bereken eerste afgeleide + zet om in absolute\n",
    "df[\"angle_deg_diff\"] = df[\"angle_deg\"].diff().abs()\n",
    "## Gezien 0 / 360 allebei het noorden zijn trekken we 360 graden van het totaal bij punten boven de 180 graden, zodat met eenheden onder de 180 graden\n",
    "df.loc[df[\"angle_deg_diff\"] > 180, \"angle_deg_diff\"] = (df.loc[df[\"angle_deg_diff\"] > 180, \"angle_deg_diff\"] - 360).abs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 3/3 feature koers: ingewikkelder formule om compass bearing uit te rekenen\n",
    "## Ik snap niet helemaal wat hij doet\n",
    "\n",
    "# De eerste functie is een wat uit\n",
    "def calculate_initial_compass_bearing(df):\n",
    "#     Calculates the bearing between two points.\n",
    "#     The formulae used is the following:\n",
    "#         θ = atan2(sin(Δlong).cos(lat2),\n",
    "#                   cos(lat1).sin(lat2) − sin(lat1).cos(lat2).cos(Δlong))\n",
    "#     :Parameters:\n",
    "#       - `pointA: The tuple representing the latitude/longitude for the\n",
    "#         first point. Latitude and longitude must be in decimal degrees\n",
    "#       - `pointB: The tuple representing the latitude/longitude for the\n",
    "#         second point. Latitude and longitude must be in decimal degrees\n",
    "#     :Returns:\n",
    "#       The bearing in degrees\n",
    "#     :Returns Type:\n",
    "#       float\n",
    "\n",
    "#     if (type(pointA) != tuple) or (type(pointB) != tuple):\n",
    "#         raise TypeError(\"Only tuples are supported as arguments\")\n",
    "#     lat = df[]\n",
    "    lat = df[\"t_latitude\"].iloc[:-1]\n",
    "    lat2 = df[\"t_latitude\"].shift(-1).dropna()\n",
    "    lat = np.asarray(lat)\n",
    "    lat2 = np.asarray(lat2)\n",
    "    lon = df[\"t_longitude\"].iloc[:-1]\n",
    "    lon2 = df[\"t_longitude\"].shift(-1).dropna()\n",
    "    lon = np.asarray(lon)\n",
    "    lon2 = np.asarray(lon2)\n",
    "\n",
    "    diffLong = np.radians(lon2 - lon)\n",
    "\n",
    "    x = np.sin(diffLong) * np.cos(lat2)\n",
    "    y = np.cos(lat) * np.sin(lat2) - (np.sin(lat)\n",
    "            * np.cos(lat2) * np.cos(diffLong))\n",
    "\n",
    "    initial_bearing = np.arctan2(x, y)\n",
    "\n",
    "#     # Now we have the initial bearing but math.atan2 return values\n",
    "#     # from -180° to + 180° which is not what we want for a compass bearing\n",
    "#     # The solution is to normalize the initial bearing as shown below\n",
    "    initial_bearing = np.degrees(initial_bearing)\n",
    "    df = df.iloc[:-1]\n",
    "    df[\"compass_bearing\"] = (initial_bearing + 360) % 360\n",
    "    \n",
    "    # Bereken eerste afgeleide + zet om in absolute\n",
    "    df[\"compass_dff\"] = df[\"compass_bearing\"].diff().abs()\n",
    "    ## Gezien 0 / 360 allebei het noorden zijn trekken we 360 graden van het totaal bij punten boven de 180 graden, zodat met eenheden onder de 180 graden\n",
    "    df.loc[df[\"compass_dff\"] > 180, \"compass_dff\"] = (df.loc[df[\"compass_dff\"] > 180, \"compass_dff\"] - 360).abs()\n",
    "\n",
    "    return(df)\n",
    "\n",
    "df = calculate_initial_compass_bearing(df=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sla csv op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## NAN door verschil berekenen koers\n",
    "df = df.dropna(subset = [\"t_orientation_dff\"]).reset_index(drop=True)\n",
    "## Wellicht zitten NAN's in de t_latitude/ t_longitude/ t_navstatus\n",
    "df = df.dropna(subset = [\"t_latitude\", \"t_longitude\"]).reset_index(drop=True)\n",
    "df = df.dropna(subset = [\"t_navstatus\"]).reset_index(drop=True)\n",
    "df[\"t_navstatus\"] = df[\"t_navstatus\"].astype(int)\n",
    "## Maak csv van preprocessing\n",
    "df.to_csv(\"sample_AIS_preprocessing.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
